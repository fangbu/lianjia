{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "civilian-prophet",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c06edd217786>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rational-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spiderSecHouse(addresslist, headers, houselist, cnt):\n",
    "    for address in addresslist:\n",
    "        print(\"开始抓取\"+address+\"区域的成交记录:\")\n",
    "        for i in range(1, 100):\n",
    "            print(\"开始抓取\" + address + \"区域第\"+str(i)+\"页的成交记录:\")\n",
    "            if (i == 1):\n",
    "                url = \"https://sh.lianjia.com/ershoufang/\" + address + \"/ie2bt2sf1ba66ea120bp300ep530/\"\n",
    "            else:\n",
    "                url = \"https://sh.lianjia.com/ershoufang/\" + address + \"/pg\" + str(i) + \"ie2bt2sf1ba66ea120bp300ep530/\"\n",
    "            page = req.get(url, headers=headers)\n",
    "            print(url + \"页面状态码：{0}\".format(page.status_code))\n",
    "            soup = BeautifulSoup(page.text, \"html.parser\")\n",
    "\n",
    "            listContent = soup.find_all(attrs={\"class\": \"sellListContent\"})\n",
    "            if(len(listContent) == 0 ):\n",
    "                break\n",
    "            ullist = listContent[0].find_all(name='li')\n",
    "\n",
    "            if(len(ullist) == 0):\n",
    "                print(\"该页内容为空，跳过\")\n",
    "                break\n",
    "            for ul in ullist:\n",
    "                print(\"已经爬取\" + str(cnt)+\"条二手房在售信息\")\n",
    "                cnt += 1\n",
    "                ifimg = ul.find_all(\"img\")\n",
    "                if (len(ifimg) == 2):\n",
    "                    img_area = ul.find_all(\"img\")[1].get('alt') #具体区域连\n",
    "                else:\n",
    "                    img_area = \"\"\n",
    "                \n",
    "                infolist = ul.find_all(attrs={\"class\": \"info\"})\n",
    "                #网页地址\n",
    "                href_connect = infolist[0].find_all(attrs={\"class\": \"\"})[0]\n",
    "                lianjie = href_connect['href']\n",
    "                # 标题，区域连\n",
    "                \n",
    "                alist = infolist[0].find_all(name='a')[0]\n",
    "                split_titleinfo = alist.string.split(\" \")\n",
    "\n",
    "                if (len(split_titleinfo) == 3):\n",
    "                    biaoti = split_titleinfo[0]\n",
    "                    huxing = split_titleinfo[1]\n",
    "                    area = split_titleinfo[2]\n",
    "                else:\n",
    "                    biaoti = split_titleinfo[0]\n",
    "                    huxing = \"\"\n",
    "                    area = img_area\n",
    "                # 楼层、房屋建设时间\n",
    "                #positionInfo = infolist[0].find(attrs={'class': \"positionInfo\"}).find('a').get_text()\n",
    "                xiaoqu = infolist[0].find(attrs={'class': \"positionInfo\"}).find_all('a')[0].string\n",
    "                positionInfo = infolist[0].find(attrs={'class': \"positionInfo\"}).find_all('a')[1].string\n",
    "                #positionIcon = positionInfo[0].get_text().split(\" \")\n",
    "                #floor = positionIcon[0]\n",
    "                #years = positionIcon[1]                \n",
    "                \n",
    "                # 房屋信息\n",
    "\n",
    "                houseInfo = infolist[0].find_all(attrs={'class': \"houseInfo\"})\n",
    "                split_houseinfo = houseInfo[0].get_text().split(\" | \")\n",
    "                if(len(split_houseinfo) == 7):\n",
    "                    fangxing = split_houseinfo[0]\n",
    "                    mianji = split_houseinfo[1]\n",
    "                    chaoxiang = split_houseinfo[2]\n",
    "                    zxtype = split_houseinfo[3]\n",
    "                    floor = split_houseinfo[4]\n",
    "                    year = split_houseinfo[5]\n",
    "                    louxing = split_houseinfo[6]\n",
    "                else:\n",
    "                    fangxing = split_houseinfo[0]\n",
    "                    mianji = split_houseinfo[1]\n",
    "                    chaoxiang = split_houseinfo[2]\n",
    "                    zxtype = split_houseinfo[3]\n",
    "                    floor = split_houseinfo[4]\n",
    "                    year = houseInfo[0].get_text()\n",
    "                    louxing = ''\n",
    "                # 关注人数，挂牌时间\n",
    "                followInfo = infolist[0].find(attrs={'class':\"followInfo\"}).text.split(\"/\")\n",
    "                follow_nums = followInfo[0]\n",
    "                guapai_time = followInfo[1]\n",
    "                # 是否满五唯一\n",
    "                five = infolist[0].find_all(attrs={'class':\"five\"})\n",
    "                taxfree = infolist[0].find_all(attrs={'class':\"taxfree\"})\n",
    "                #print(infolist[0])\n",
    "                if(len(five) == 0 ):\n",
    "                    if(len(taxfree) == 0 ):\n",
    "                        manwu = \"\"\n",
    "                    else:\n",
    "                        manwu = taxfree[0].string\n",
    "                else:\n",
    "                    manwu = five[0].string\n",
    "                # 成交总价\n",
    "                totalPrice = infolist[0].find(attrs={'class':\"totalPrice\"}).find('span').string\n",
    "                # 单价\n",
    "                unitPrice = infolist[0].find(attrs={'class':\"unitPrice\"}).find('span').string\n",
    "                # 挂牌价格、成交周期\n",
    "                #dealCycleeInfo = infolist[0].find_all(attrs={'class': \"dealCycleeInfo\"})\n",
    "                #dealCycleTxt = dealCycleeInfo[0].find_all(attrs={'class': \"dealCycleTxt\"})\n",
    "                #dealCycleTxt_span = dealCycleTxt[0].find_all(\"span\")\n",
    "                #if (len(dealCycleTxt_span) == 2):\n",
    "                #    listPrice = dealCycleTxt_span[0].get_text()\n",
    "                #    dealCycle = dealCycleTxt_span[1].get_text()\n",
    "                #elif (len(dealCycleTxt_span) == 1):\n",
    "                #    if (\"成交周期\" in dealCycleTxt_span[0].get_text()):\n",
    "                #        dealCycle = dealCycleTxt_span[0].get_text()\n",
    "                #        listPrice = \"\"\n",
    "                #    if (\"挂牌\" in dealCycleTxt_span[0].get_text()):\n",
    "                #        listPrice = dealCycleTxt_span[0].get_text()\n",
    "                #        dealCycle = \"\"\n",
    "\n",
    "                houselist.append(\n",
    "                    [address, biaoti,huxing,area,xiaoqu,positionInfo,fangxing,mianji,chaoxiang,zxtype,floor,year,louxing, \\\n",
    "                     follow_nums,guapai_time,manwu,totalPrice,unitPrice,lianjie])  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-visiting",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    headers = {\n",
    "        'User-Agent':\n",
    "            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \\\n",
    "                                   (KHTML, like Gecko) Chrome/86.0.4240.75 Safari/537.36'\n",
    "    }\n",
    "    # 浦东、杨浦、宝山\n",
    "    addresslist = [\"pudong\", \"minhang\", \"baoshan\", \"xuhui\", \"putuo\", \"yangpu\", \"changning\", \"songjiang\", \"jiading\", \\\n",
    "                   \"huangpu\", \"jingan\", \"hongkou\", \"qingpu\"]\n",
    "    # 最终爬取的成交信息\n",
    "    houselist = []\n",
    "    # 总记录数\n",
    "    cnt = 0\n",
    "\n",
    "    spiderSecHouse(addresslist, headers, houselist, cnt)\n",
    "\n",
    "    #保存为csv\n",
    "    columns = [\"address\",\"biaoti\",\"huxing\",\"area\",\"xiaoqu\",\"positionInfo\",\"fangxing\",\"mianji\",\"chaoxiang\",\"zxtype\",\"floor\", \\\n",
    "               \"year\",\"louxing\",\"follow_nums\",\"guapai_time\",\"manwu\",\"totalPrice\",\"unitPrice\",\"lianjie\"] #\n",
    "\n",
    "    df = pd.DataFrame(columns=columns, data=houselist)\n",
    "    df.to_excel('lianjia_test1.xlsx')\n",
    "    #df.to_csv(\"secHand_add.csv\", mode='a', encoding=\"gbk\")\n",
    "\n",
    "    #secHand.csv 不带区域信息\n",
    "    #secHand_address.csv 带区域信息 挂牌价格 成交周期有误\n",
    "    #secHand_add.csv 带区域信息 挂牌价格 成交周期无误"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
